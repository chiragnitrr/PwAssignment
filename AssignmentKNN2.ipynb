{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c37f24-2461-442f-bfda-f7e291cfc6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance metric in KNN? How might this\n",
    "difference affect the performance of a KNN classifier or regressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3228fe79-b95e-430e-8d9b-cd2f05b15026",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer :\n",
    "    The main difference between the Euclidean distance metric and the Manhattan distance metric lies in the way they measure the\n",
    "    distance between two points in a multidimensional space. These differences can have an impact on the performance of a KNN (k-\n",
    "    Nearest Neighbors) classifier or regressor, depending on the characteristics of the data. Here's a summary of the distinctions \n",
    "    and their potential effects:\n",
    "    \n",
    "## Key Differences:\n",
    "1.Sensitivity to Dimensions:\n",
    "- Euclidean distance is more sensitive to differences in magnitude between dimensions. Larger differences in one dimension have a \n",
    "greater impact on the overall distance.\n",
    "- Manhattan distance treats differences in each dimension equally, making it less sensitive to variations in magnitude.\n",
    "\n",
    "2.Path of Measurement:\n",
    "- Euclidean distance measures the shortest straight-line path between two points.\n",
    "- Manhattan distance measures the distance traveled along the gridlines in a horizontal and vertical direction.\n",
    "\n",
    "3.Geometry:\n",
    "- Euclidean distance is associated with the geometric interpretation of the straight-line distance.\n",
    "- Manhattan distance is associated with the geometric interpretation of the distance traveled on a grid-like path.\n",
    "\n",
    "4.Impact of Outliers:\n",
    "- Euclidean distance can be sensitive to outliers, especially when the magnitude of the differences is large.\n",
    "- Manhattan distance is less affected by outliers since it only considers the absolute differences.\n",
    "\n",
    "## Choosing Between Euclidean and Manhattan Distance in KNN:\n",
    "\n",
    "1. Euclidean Distance:\n",
    "- Often used when the data points represent continuous variables and when the relationships between dimensions are expected to be \n",
    "isotropic (similar in all directions).\n",
    "- Suitable when the differences in magnitude between dimensions are relevant to the problem.\n",
    "\n",
    "2. Manhattan Distance:\n",
    "- Can be a good choice when dealing with categorical variables or when dimensions are not directly comparable in magnitude.\n",
    "- Suitable when the problem involves grid-like movement, such as in city block distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84481096-6a95-486c-9c76-8a132fb1ef67",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be used to determine the \n",
    "optimal k value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d19160-7219-4ea7-a580-a3523a7975c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer :\n",
    "Choosing the optimal value of 'k' in a KNN (k-Nearest Neighbors) classifier or regressor is a crucial step that can significantly\n",
    "impact the model's performance. The selection of 'k' depends on the characteristics of the dataset and the specific problem you are\n",
    "trying to solve. Here are several techniques commonly used to determine the optimal 'k' value:\n",
    "\n",
    "1. Grid Search:\n",
    "- Perform a grid search over a range of 'k' values and evaluate the model's performance using a validation set or cross-validation.\n",
    "- Choose the 'k' value that results in the best performance according to a chosen metric (e.g., accuracy, mean squared error).\n",
    "\n",
    "2. Cross-Validation:\n",
    "- Use cross-validation, such as k-fold cross-validation, to assess the model's performance for different 'k' values.\n",
    "- Calculate the average performance metric (e.g., accuracy, mean squared error) over multiple folds for each 'k' value.\n",
    "- Choose the 'k' that provides the best trade-off between bias and variance.\n",
    "\n",
    "3. Elbow Method:\n",
    "- For regression tasks, plot the mean squared error or, for classification tasks, plot the error rate against different 'k' values.\n",
    "- Look for the point where the error starts to decrease at a slower rate, forming an \"elbow\" on the graph.\n",
    "- This point represents a good trade-off between model complexity and performance.\n",
    "\n",
    "4. Leave-One-Out Cross-Validation (LOOCV):\n",
    "- Perform LOOCV, a special case of k-fold cross-validation where 'k' is set to the number of instances in the dataset.\n",
    "- Evaluate the model's performance for each 'k' value by leaving out one data point at a time for testing.\n",
    "- Calculate the average performance metric over all iterations for each 'k' value.\n",
    "\n",
    "5. Use Domain Knowledge:\n",
    "- Consider any domain-specific knowledge that might guide the choice of 'k.'\n",
    "- For example, if the decision boundary is expected to be smooth, a smaller 'k' might be appropriate, while a larger 'k' may be \n",
    "suitable for more complex decision boundaries.\n",
    "\n",
    "6. Odd vs. Even 'k':\n",
    "- In binary classification problems, choose an odd value for 'k' to avoid ties when voting for the class label. Odd values help \n",
    "ensure a clear majority.\n",
    "\n",
    "7. Weighted KNN:\n",
    "- In some cases, use weighted KNN where the influence of each neighbor is weighted by its distance.\n",
    "- Experiment with different distance weightings to find the optimal balance.\n",
    "\n",
    "8. Randomized Search:\n",
    "- Instead of exhaustively searching over a predefined range, perform a randomized search over a range of 'k' values.\n",
    "- This approach may be more efficient, especially when the search space is large.\n",
    "\n",
    "9. Automated Hyperparameter Tuning:\n",
    "- Utilize automated hyperparameter tuning tools and libraries (e.g., scikit-learn's GridSearchCV or RandomizedSearchCV) to \n",
    "systematically search for the optimal 'k' value.\n",
    "                                                               \n",
    "Considerations:\n",
    "- It's important to use an appropriate evaluation metric for your specific problem (e.g., accuracy, precision, recall, mean squared\n",
    "error) when assessing the performance of different 'k' values.\n",
    "- The optimal 'k' value may vary depending on the characteristics of the dataset, so it's recommended to try different techniques \n",
    "and validate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0e9701-1777-416f-8b81-b7b2e79a1796",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In what situations might you\n",
    "choose one distance metric over the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6e6137-aeb1-4a25-ac20-9c2f2f1a50c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer :\n",
    "\n",
    "The choice of distance metric in a KNN (k-Nearest Neighbors) classifier or regressor is a critical aspect that can significantly \n",
    "influence the model's performance. The distance metric determines how the similarity or dissimilarity between data points is measured,\n",
    "and different metrics may be more suitable for specific types of data or problems. Two common distance metrics are Euclidean distance \n",
    "and Manhattan distance, but there are others such as Minkowski, Chebyshev, and Hamming distance. Here's how the choice of distance \n",
    "metric can impact performance and situations where one metric might be preferred over the other:\n",
    "\n",
    "1. Euclidean Distance:\n",
    "- Characteristics:\n",
    "  - Measures the straight-line distance between two points in a multidimensional space.\n",
    "  - Reflects the geometric interpretation of the shortest path between two points.\n",
    "  - Sensitive to differences in magnitude between corresponding coordinates.\n",
    "\n",
    "- Suitable Situations:\n",
    "  - Appropriate when relationships between features are isotropic (similar in all directions).\n",
    "  - Effective for data where the concept of proximity is best represented by a straight-line distance.\n",
    "\n",
    "2. Manhattan Distance (L1 Norm or Taxicab Distance):\n",
    "- Characteristics:\n",
    "  - Measures the distance based on the sum of the absolute differences between corresponding coordinates.\n",
    "  - Represents the distance traveled along gridlines in a horizontal and vertical direction.\n",
    "  - Less sensitive to differences in magnitude between corresponding coordinates.\n",
    "\n",
    "- Suitable Situations:\n",
    "  - Appropriate when relationships between features are anisotropic (differ in different directions).\n",
    "  - Effective when features are measured on different scales, and differences in magnitude are less relevant.\n",
    "  - Suitable for cases where straight-line distance may not accurately represent proximity.\n",
    "\n",
    "3. Minkowski Distance:\n",
    "- Characteristics:\n",
    "  - Generalization of Euclidean and Manhattan distances.\n",
    "  - Parameterized by a value 'p,' and when 'p' is 1, it is equivalent to Manhattan distance, and when 'p' is 2, it is equivalent to\n",
    "    Euclidean distance.\n",
    "    \n",
    "- Suitable Situations:\n",
    "  - Allows for flexibility by adjusting the value of 'p' based on the problem characteristics.\n",
    "  - Suitable for scenarios where a hybrid approach between Euclidean and Manhattan distances is desirable.\n",
    "    \n",
    "4. Chebyshev Distance:\n",
    "- Characteristics:\n",
    "  - Measures the maximum absolute difference between corresponding coordinates.\n",
    "\n",
    "- Suitable Situations:\n",
    "  - Suitable for problems where only the largest difference between coordinates matters.\n",
    "  - Effective for situations where outliers may have a significant impact on the distance metric.\n",
    "    \n",
    "5. Hamming Distance:\n",
    "- Characteristics:\n",
    "  - Specifically designed for categorical data.\n",
    "  - Measures the number of positions at which the corresponding symbols are different.\n",
    "    \n",
    "- Suitable Situations:\n",
    "  - Appropriate for problems involving categorical features or binary data.\n",
    "  - Effective when the focus is on feature-wise agreement or disagreement.\n",
    "\n",
    "## Considerations for Choosing Distance Metric:\n",
    "1. Nature of Data:\n",
    "- Consider the nature of your data—whether it's continuous, categorical, or a mix of both.\n",
    "- Euclidean and Manhattan distances are suitable for continuous data, while Hamming distance is designed for categorical data.\n",
    "\n",
    "2. Feature Characteristics:\n",
    "- Assess the characteristics of the features, including their scales and distributions.\n",
    "- If features have similar scales and relationships in all directions, Euclidean distance might be appropriate. If scales differ \n",
    "or relationships are anisotropic, Manhattan distance may be preferred.\n",
    "\n",
    "3. Problem Requirements:\n",
    "- Consider the specific requirements of your problem and the type of relationships between data points.\n",
    "- Experiment with different distance metrics and choose the one that aligns with the problem's characteristics.\n",
    "\n",
    "4. Empirical Evaluation:\n",
    "- Experiment with multiple distance metrics and evaluate their impact on model performance using techniques like cross-validation.\n",
    "\n",
    "5. Domain Knowledge:\n",
    "- Incorporate domain knowledge, when available, to guide the selection of a distance metric that aligns with the problem's \n",
    "characteristics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee93d6e-0015-4922-a3d3-097d3c25870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect the performance of the model? How \n",
    "might you go about tuning these hyperparameters to improve model performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c2f11a-a229-4d47-a603-940862840dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer :\n",
    "    KNN (k-Nearest Neighbors) classifiers and regressors have hyperparameters that can be tuned to optimize model performance. The\n",
    "    key hyperparameter in KNN is the number of neighbors ('k'), but there are additional parameters that can influence the behavior \n",
    "    of the algorithm. Here are some common hyperparameters in KNN and their impact on model performance:\n",
    "\n",
    "1. Number of Neighbors ('k'):\n",
    "- Hyperparameter: The number of nearest neighbors considered when making predictions.\n",
    "- Impact: Affects the model's bias-variance trade-off. Smaller 'k' values lead to more complex decision boundaries, which may be \n",
    "sensitive to noise. Larger 'k' values result in smoother decision boundaries but might miss local patterns.\n",
    "- Tuning: Perform cross-validation or other model evaluation techniques to find the optimal 'k' value that balances bias and variance.\n",
    "\n",
    "2. Distance Metric:\n",
    "- Hyperparameter: The metric used to calculate the distance between data points (e.g., Euclidean distance, Manhattan distance).\n",
    "- Impact: The choice of distance metric affects how similarity or dissimilarity between data points is measured. Different metrics\n",
    "may be more suitable for specific types of data or relationships.\n",
    "- Tuning: Experiment with different distance metrics and choose the one that provides better results through cross-validation or \n",
    "empirical evaluation.\n",
    "\n",
    "3. Weighting Scheme:\n",
    "- Hyperparameter: Determines how the contributions of neighbors are weighted when making predictions. Options include uniform (equal\n",
    " weights) and distance-based (weights inversely proportional to distance).\n",
    "- Impact: Weighting affects the influence of neighbors on the prediction. Distance-based weighting gives more importance to closer\n",
    "neighbors.\n",
    "- Tuning: Experiment with different weighting schemes and evaluate their impact on model performance using cross-validation.\n",
    "\n",
    "4. Algorithm:\n",
    "- Hyperparameter: Specifies the algorithm used to compute nearest neighbors. Common options include 'auto,' 'ball_tree,' 'kd_tree,' \n",
    "and 'brute.'\n",
    "- Impact: Different algorithms have varying computational complexities and may perform differently based on the dataset size and \n",
    "dimensionality.\n",
    "- Tuning: Experiment with different algorithms and choose the one that balances computational efficiency with model performance.\n",
    "\n",
    "5. Leaf Size:\n",
    "- Hyperparameter: The size of the leaf nodes in the KD tree or Ball tree algorithms. Affects the trade-off between speed and memory\n",
    "usage.\n",
    "- Impact: Larger leaf sizes can reduce the number of distance calculations but may result in less accurate predictions.\n",
    "- Tuning: Adjust the leaf size based on the dataset size and dimensionality to find a balance between computational efficiency and\n",
    "model accuracy.\n",
    "\n",
    "6. Parallelization:\n",
    "- Hyperparameter: Controls whether the algorithm is parallelized for faster computation. Common options include 'n_jobs' or \n",
    "'n_jobs=-1' for parallel processing.\n",
    "- Impact: Parallelization can significantly speed up computations, especially for large datasets.\n",
    "- Tuning: Experiment with parallelization options, considering the available hardware resources.\n",
    "\n",
    "7. P: Minkowski Power Parameter:\n",
    "- Hyperparameter: Applies to the Minkowski distance metric and determines the power parameter ('p'). When 'p' is 1, it is equivalent\n",
    "to Manhattan distance, and when 'p' is 2, it is equivalent to Euclidean distance.\n",
    "- Impact: Adjusts the sensitivity of the distance metric to differences in magnitude between coordinates.\n",
    "- Tuning: Experiment with different values of 'p' based on the nature of the data and relationships between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c9394f-ee94-4919-82db-1ec03779bfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What techniques can be used to\n",
    "optimize the size of the training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2ca31d-166c-4e35-8d3c-e9b72d7f88f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer :\n",
    "The size of the training set can have a significant impact on the performance of a KNN (k-Nearest Neighbors) classifier or regressor.\n",
    "The size of the training set influences the algorithm's ability to generalize well to unseen data, and finding the right balance is \n",
    "crucial for achieving optimal performance. Here are some considerations regarding the impact of the training set size and techniques \n",
    "to optimize it:\n",
    "\n",
    "# Impact of Training Set Size:\n",
    "1. Small Training Set:\n",
    "- Advantages:\n",
    "  - Computationally less expensive.\n",
    "  - Faster training times.\n",
    "  - Can be suitable for simpler models or datasets with fewer patterns.\n",
    "\n",
    "- Challenges:\n",
    "  - Increased sensitivity to noise and outliers.\n",
    "  - Greater risk of overfitting, especially with smaller values of 'k.'\n",
    "  - Decision boundaries may be less robust, leading to poor generalization.\n",
    "\n",
    "2. Large Training Set:\n",
    "- Advantages:\n",
    "  - Reduced sensitivity to noise and outliers.\n",
    "  - Smoother decision boundaries, promoting better generalization.\n",
    "  - Increased ability to capture complex patterns in the data.\n",
    "\n",
    "- Challenges:\n",
    "  - Computationally more expensive, especially during prediction.\n",
    "  - Slower training times.\n",
    "  - May become impractical for very large datasets due to the computational burden.\n",
    "\n",
    "# Techniques to Optimize Training Set Size:\n",
    "1. Cross-Validation:\n",
    "- Use cross-validation to assess the impact of different training set sizes on the model's performance.\n",
    "- Evaluate the model's accuracy, precision, recall, or other relevant metrics across various training set sizes.\n",
    "- Choose a size that provides the best trade-off between bias and variance.\n",
    "\n",
    "2. Learning Curves:\n",
    "- Plot learning curves that depict the model's performance (e.g., accuracy or error) against different training set sizes.\n",
    "- Observe how the performance stabilizes as the training set size increases.\n",
    "- Identify the point where further increases in the training set size have diminishing returns.\n",
    "\n",
    "3. Incremental Learning:\n",
    "- Implement incremental or online learning techniques to update the model as new data becomes available.\n",
    "- This is particularly useful when dealing with streaming data or situations where collecting a large labeled dataset at once is \n",
    "challenging.\n",
    "\n",
    "4. Feature Selection/Dimensionality Reduction:\n",
    "- If the dataset is large but high-dimensional, consider feature selection or dimensionality reduction techniques.\n",
    "- Reducing the number of features can lead to a more manageable and informative training set.\n",
    "\n",
    "5. Stratified Sampling:\n",
    "- Use stratified sampling to ensure that the distribution of classes in the training set reflects the overall distribution in the\n",
    "dataset.\n",
    "- This helps prevent biases and ensures that the model is exposed to representative examples of each class.\n",
    "\n",
    "6. Data Augmentation:\n",
    "- Augment the training set by creating additional synthetic samples through techniques like rotation, flipping, or adding noise.\n",
    "- This is especially useful when dealing with image or text data.\n",
    "\n",
    "7. Active Learning:\n",
    "- Implement active learning strategies to iteratively select the most informative samples for labeling.\n",
    "- This can help maximize the utility of the training set by focusing on the instances that contribute the most to improving the \n",
    "model's performance.\n",
    "\n",
    "8. Ensemble Methods:\n",
    "- Use ensemble methods to combine predictions from multiple models trained on different subsets of the training data.\n",
    "- This can provide a way to leverage diverse subsets of the data, potentially improving overall model performance.\n",
    "\n",
    "9. Parallelization:\n",
    "- If computational resources permit, parallelize the training process to handle larger training sets efficiently.\n",
    "- Parallelization can be achieved using tools and frameworks that support distributed computing.\n",
    "\n",
    "10. Evaluate Trade-offs:\n",
    "- Consider the trade-offs between model performance and computational cost.\n",
    "- Evaluate whether the benefits of a larger training set justify the increased computational requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b4647f-0fbf-42b7-9a80-f8690c8b18d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you overcome these drawbacks to improve \n",
    "the performance of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bf6a0b-db5e-4437-afe7-4c1431c4d892",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer :\n",
    "    \n",
    "While k-Nearest Neighbors (KNN) is a simple and intuitive algorithm, it comes with certain drawbacks that can impact its \n",
    "performance in certain scenarios. Here are some potential drawbacks of using KNN as a classifier or regressor, along with strategies \n",
    "to overcome these drawbacks:\n",
    "\n",
    "1. Computational Complexity:\n",
    "Drawback:\n",
    "- Calculating distances between all data points becomes computationally expensive, especially with large datasets.\n",
    "- The time complexity for prediction can be high, especially when the dataset has many dimensions.\n",
    "\n",
    "Overcoming:\n",
    "- Use data structures like KD-trees or Ball trees to accelerate nearest neighbor searches.\n",
    "- Implement algorithms that are optimized for efficient nearest neighbor retrieval, such as the K-D tree or Ball tree-based \n",
    "implementations in scikit-learn.\n",
    "- Consider parallelizing computations when dealing with large datasets and multicore architectures.\n",
    "\n",
    "2. Sensitivity to Noise and Outliers:\n",
    "Drawback:\n",
    "- KNN can be sensitive to noisy data and outliers, as they can significantly impact the identification of nearest neighbors.\n",
    "\n",
    "Overcoming:\n",
    "- Apply data preprocessing techniques to handle outliers, such as outlier detection and removal.\n",
    "- Use distance metrics that are less sensitive to outliers, like Manhattan distance or Minkowski distance with a low 'p' value.\n",
    "- Experiment with different weighting schemes to give less influence to distant or outlier neighbors.\n",
    "\n",
    "3. Curse of Dimensionality:\n",
    "Drawback:\n",
    "- KNN performance degrades as the number of dimensions increases due to the curse of dimensionality.\n",
    "- In high-dimensional spaces, the concept of proximity becomes less meaningful.\n",
    "\n",
    "Overcoming:\n",
    "- Apply dimensionality reduction techniques, such as Principal Component Analysis (PCA), to reduce the number of features.\n",
    "- Use feature selection to choose relevant features and eliminate irrelevant ones.\n",
    "- Consider algorithms designed for high-dimensional spaces, such as Locality-Sensitive Hashing (LSH) for approximate nearest neighbor \n",
    "search.\n",
    "\n",
    "4. Need for Feature Scaling:\n",
    "Drawback:\n",
    "- Features with larger scales can dominate the distance calculations.\n",
    "- Feature scaling is often required to ensure equal importance for all features.\n",
    "\n",
    "Overcoming:\n",
    "- Normalize or standardize features to bring them to a similar scale.\n",
    "- Use scaling techniques like Min-Max scaling or Z-score normalization to make features comparable.\n",
    "\n",
    "5. Choosing the Right 'k':\n",
    "Drawback:\n",
    "- The choice of 'k' is crucial and can impact the model's performance.\n",
    "- An inappropriate 'k' value may lead to underfitting or overfitting.\n",
    "\n",
    "Overcoming:\n",
    "- Use cross-validation to evaluate the model's performance for different 'k' values and choose the one that minimizes overfitting or\n",
    "underfitting.\n",
    "- Experiment with different values of 'k' and visualize learning curves or validation curves to identify the optimal 'k.'\n",
    "\n",
    "6. Imbalanced Datasets:\n",
    "Drawback:\n",
    "- KNN can be biased towards the majority class in imbalanced datasets.\n",
    "\n",
    "Overcoming:\n",
    "- Consider techniques such as oversampling or undersampling to balance class distribution.\n",
    "- Use distance weighting or adjust class weights to give more importance to minority classes.\n",
    "\n",
    "7. Memory Usage:\n",
    "Drawback:\n",
    "- KNN requires storing the entire training dataset in memory for prediction.\n",
    "\n",
    "Overcoming:\n",
    "- Use data structures like KD-trees or Ball trees that are memory-efficient for nearest neighbor retrieval.\n",
    "- Implement approximate nearest neighbor search algorithms that trade accuracy for reduced memory requirements.\n",
    "\n",
    "8. Noisy Data and Local Optima:\n",
    "Drawback:\n",
    "- KNN may struggle with datasets containing complex decision boundaries or regions where class labels change rapidly.\n",
    "\n",
    "Overcoming:\n",
    "- Use ensemble methods or combine KNN with other algorithms to improve robustness.\n",
    "- Implement techniques like local averaging or weighted voting to reduce the impact of noisy data.\n",
    "\n",
    "9. Large Search Spaces:\n",
    "Drawback:\n",
    "- In high-dimensional spaces, the search space for finding nearest neighbors becomes large, leading to increased computational \n",
    "complexity.\n",
    "\n",
    "Overcoming:\n",
    "- Use dimensionality reduction techniques to reduce the number of features.\n",
    "- Implement localized versions of KNN, such as Radius Neighbors Classifier/Regressor, to focus on local patterns.\n",
    "\n",
    "10. Handling Missing Values:\n",
    "Drawback:\n",
    "- KNN struggles with missing values in the dataset.\n",
    "\n",
    "Overcoming:\n",
    "- Impute missing values using techniques like mean imputation or KNN-based imputation before applying KNN.\n",
    "- Consider using algorithms that are more robust to missing data or handle missing values explicitly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
