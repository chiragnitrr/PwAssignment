{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beb45425-6184-4544-be0f-db6857e1cd7e",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0949a94-bddf-4207-9faf-e048e7ea89e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer : Min-Max scaling, also known as normalizationm, is a data processing technique used in machine learning to transform\n",
    "         features or variables in a dataset so that they are on common. scale This process involves scaling the values of each\n",
    "         feature to a specific range ususlly 0 to 1 . Min -Max scalling is particularly useful when the features in a dataset \n",
    "         have varying scales, and you want to ensure that each feature contributes equally to the learning process.\n",
    "         \n",
    "        formula : x(scaled) = [x - min(x)]/[max(x) - min(x)]\n",
    "        Person  \tAge\t    Income\n",
    "           A\t    25\t    50000\n",
    "           B\t    40\t    80000\n",
    "           C\t    30\t    60000\n",
    "           D\t    22\t    45000\n",
    "           E\t    35   \t70000     \n",
    "            \n",
    "        using the given formula new table will be ;\n",
    "        Person\t      Scaled Age\t     Scaled Income\n",
    "         A\t            0.17\t          0.33\n",
    "         B            \t1.00\t          1.00\n",
    "         C           \t0.44\t          0.56\n",
    "         D           \t0.00\t          0.00\n",
    "         E           \t0.72\t          0.78"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7441df6c-f858-483c-9dfe-dc988c3e22ed",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to \n",
    "    illustrate its application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e387fa-f82a-4aa2-bbef-b363f20194fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer : The unit vector technique, also known as \"Normalization\", is a feature scaling method that involves transforming the values\n",
    "         of each feature in a dataset to have a unit norm. In other words, each feature vector is scaled so that its euclidean norm\n",
    "         (magnitude) becomes 1. This technique is particularly useful when you want to emphasize the direction of the data points \n",
    "         rather than their magnitude. The primary goal of unit vector scaling is to ensure that the scaled feature vectors lie on\n",
    "         the surface of a unit sphere.\n",
    "        \n",
    "         Formula : x(scaled) = x / ||x||\n",
    "            ||x|| represents the magnitude of the feature vector x.\n",
    "        \n",
    "        unlike Min-Max scalling, which scales feature values to a specific range , unit vector scaling focuses on the direction\n",
    "        of the feature vectors while maintaining their original relationship in terms of direction.\n",
    "        \n",
    "        Example :\n",
    "         Person A: Height = 160 cm, Weight = 50 kg\n",
    "         Person B: Height = 175 cm, Weight = 70 kg\n",
    "         Person C: Height = 180 cm, Weight = 80 kg\n",
    "        \n",
    "        ∥Person A∥  =  167.97\n",
    "        ∥Person B∥  =  187.08\n",
    "        ∥Person C∥  =  193.24\n",
    "        \n",
    "        Person A: Scaled Height = 160/167.97 = 0.952, Scaled Weight = 50/167.97 = 0.298\n",
    "        Person B: Scaled Height= 175/187.08 = 0.94, Scaled Weight = 0.374\n",
    "        Person C: Scaled Height = 180/193.24 = 0.93, Scaled Weight = 80/193.24 = 0.413"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bd1936-af16-4e43-82f1-d74e28283b6b",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate\n",
    "    its application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5c4ec5-a059-4d72-9d66-43b6aba3d924",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer : Pricipal Component Analysis (PCA) is a dimensionality reduction technique used in statistics and machine learning. It main\n",
    "         objective is to transform a high - dimensional dataset into a lower-dimensional space while retaining as much of the original\n",
    "         variance as possible. This reduction in dimensionality can simplify the dataset, making it easier to visualize , analyze and \n",
    "         process, while still preserving the essential features of the data.\n",
    "            \n",
    "         PCA works by identifying the directions (principal components) in the original feature space along which the data varies the\n",
    "         most. These directions are orthogonal to each other, and the first principal component accounts for the largest variance in the \n",
    "         data, the second for the second-largest variance, and so on. By projecting the data onto these principal components, which serve\n",
    "         as new  axes, we effectively reduce the dimensionality of the data.   \n",
    "            \n",
    "         Original Data (2D) :\n",
    "            |   X   |   Y   |\n",
    "            |-------|-------|\n",
    "            |  2.5  |  3.5  |\n",
    "            |  0.5  |  0.7  |\n",
    "            |  2.2  |  2.9  |\n",
    "            |  1.9  |  2.2  |\n",
    "            |  3.1  |  3.0  |\n",
    "            |  2.3  |  2.7  |\n",
    "            |  2.0  |  1.6  |\n",
    "            |  1.0  |  1.1  |\n",
    "            |  1.5  |  1.6  |\n",
    "            |  1.1  |  0.9  |\n",
    "             Compute the mean of each feature ('X' and 'Y'):\n",
    "              mean_X = 1.81\n",
    "              mean_Y = 2.29\n",
    "\n",
    "             Center the data by subtracting the means from each data point:\n",
    "                |   X   |   Y   |\n",
    "                |-------|-------|\n",
    "                |  0.69 |  1.21 |\n",
    "                | -1.31 | -1.59 |\n",
    "                |  0.39 |  0.61 |\n",
    "                |  0.09 | -0.09 |\n",
    "                |  1.29 |  0.71 |\n",
    "                |  0.49 |  0.41 |\n",
    "                |  0.19 | -0.69 |\n",
    "                | -0.81 | -1.19 |\n",
    "                | -0.31 | -0.69 |\n",
    "                | -0.71 | -1.39 |\n",
    "\n",
    "                Calculate the covariance matrix of the centered data :\n",
    "                   covariance_matrix = | 0.61655556  0.61544444 |\n",
    "                                       | 0.61544444  0.71655556 |\n",
    " \n",
    "              Compute the eigenvalues and eigenvectors of the covariance matrix. The eigenvector corresponding to the largest\n",
    "              eigenvalue is the first principal component, and the second largest eigenvalue corresponds to the second principal\n",
    "              component.\n",
    "            \n",
    "              Project the centered data onto the first principal component :\n",
    "                    |   X   |\n",
    "                    |-------|\n",
    "                    |  1.22 |\n",
    "                    | -1.86 |\n",
    "                    |  0.11 |\n",
    "                    | -0.38 |\n",
    "                    |  1.38 |\n",
    "                    |  0.28 |\n",
    "                    | -0.94 |\n",
    "                    | -1.87 |\n",
    "                    | -1.19 |\n",
    "                    | -2.43 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f9979d-1409-477d-925c-592dd4a3d9a9",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example\n",
    "    to illustrate this concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdefac7f-afa5-4a18-bb7a-767d24667ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer : The relationship between PCA and feature extraction lies in the fact that PCA aims to find a new representation of the\n",
    "         data that retains as much information (variance) as possible while reducing the dimensionality. This can help in simplifying\n",
    "         the data, removing noise, and potentially improving the performance of machine learning algorithms that struggle with high-\n",
    "         dimensional data or multicollinearity among features.\n",
    "         \n",
    "        Suppose you have a dataset with three features: 'X', 'Y', and 'Z'. You want to use PCA for feature extraction and reduce the\n",
    "        dimensionality to two components.\n",
    "        Original Data : \n",
    "            |   X   |   Y   |   Z   |\n",
    "            |-------|-------|-------|\n",
    "            |  2.5  |  3.5  |  1.2  |\n",
    "            |  0.5  |  0.7  |  2.2  |\n",
    "            |  2.2  |  2.9  |  0.5  |\n",
    "            |  1.9  |  2.2  |  1.5  |\n",
    "            |  3.1  |  3.0  |  0.8  |\n",
    "            |  2.3  |  2.7  |  1.9  |\n",
    "            |  2.0  |  1.6  |  2.3  |\n",
    "            |  1.0  |  1.1  |  0.7  |\n",
    "            |  1.5  |  1.6  |  1.5  |\n",
    "            |  1.1  |  0.9  |  2.0  |\n",
    "\n",
    "            Step 1: Center the data by subtracting the mean of each feature.\n",
    "            Step 2: Calculate the covariance matrix of the centered data.\n",
    "            Step 3: Compute the eigenvalues and eigenvectors of the covariance matrix.\n",
    "            Step 4: Sort the eigenvectors by their corresponding eigenvalues in descending order. The eigenvectors with the\n",
    "                    largest eigenvalues are the principal components.\n",
    "                \n",
    "            the sorted eigenvectors are:\n",
    "                PC1 = [ 0.71,  0.45, -0.55]\n",
    "                PC2 = [-0.48,  0.87,  0.15]\n",
    "                PC3 = [ 0.51,  0.13,  0.85]\n",
    "\n",
    "            Step 5: Select the top two principal components (PC1 and PC2) as the new features\n",
    "            Transformed data using PCA:\n",
    "                |  PC1  |  PC2  |\n",
    "                |-------|-------|\n",
    "                |  1.57 |  0.64 |\n",
    "                | -1.90 | -0.58 |\n",
    "                |  0.19 |  1.32 |\n",
    "                | -0.24 |  0.09 |\n",
    "                |  1.54 | -0.36 |\n",
    "                |  0.84 |  0.37 |\n",
    "                |  0.81 | -0.88 |\n",
    "                | -1.13 |  0.06 |\n",
    "                | -0.36 | -0.20 |\n",
    "                | -2.03 |  0.34 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498a532a-7c3c-4067-b7ce-be58f5a294f1",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such \n",
    "    as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7fb439-7a74-4743-994a-4b4783ab2ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer :  Min-Max Scaling Formula:\n",
    "          X_scaled = (X - X_min) / (X_max - X_min)\n",
    "\n",
    "         Original data (sample):\n",
    "        | Price | Rating | Delivery Time |\n",
    "        |-------|--------|---------------|\n",
    "        |  20   |   4.5  |     30        |\n",
    "        |  15   |   3.8  |     40        |\n",
    "        |  25   |   4.2  |     25        |\n",
    "\n",
    "        Step 1: Calculate Min and Max Values\n",
    "        Price: Min = 15, Max = 25\n",
    "        Rating: Min = 3.8, Max = 4.5\n",
    "        Delivery Time: Min = 25, Max = 40\n",
    "\n",
    "        Step 2: Apply Min-Max Scaling\n",
    "        Price_scaled = (Price - 15) / (25 - 15)\n",
    "        Rating_scaled = (Rating - 3.8) / (4.5 - 3.8)\n",
    "        Delivery_Time_scaled = (Delivery_Time - 25) / (40 - 25)\n",
    "\n",
    "        Scaled data:\n",
    "            | Price_scaled | Rating_scaled | Delivery_Time_scaled |\n",
    "            |--------------|---------------|----------------------|\n",
    "            |    0.5       |     0.75      |         0.25         |\n",
    "            |    0.25      |     0.0       |         0.75         |\n",
    "            |    1.0       |     0.4       |         0.0          |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20179745-160b-4a17-aca1-ecdfe0130c91",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company \n",
    "    financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ab581f-2899-4bf8-aeef-141b2f8e8a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer : Using PCA to reduce the dimensionality of a dataset in the context of predicting stock prices can be a powerful technique.\n",
    "         It can help you handle the curse of dimensionality, mitigate multicollinearity among features, and potentially improve the\n",
    "         model's generalization performance. Here's how you would use PCA to achieve dimensionality reduction for your stock price\n",
    "         prediction project:\n",
    "\n",
    "     1.Understand the Dataset: Familiarize yourself with the features in your dataset. These features might include company-specific\n",
    "       financial data (e.g., revenue, earnings, debt), market trends (e.g., interest rates, inflation), and other relevant factors.\n",
    "     2.Preprocess the Data: Handle missing values, outliers, and any other data preprocessing steps that are required before applying\n",
    "       PCA.\n",
    "     3.Standardize the Data: Since PCA is sensitive to the scale of the features, it's a good practice to standardize the data by \n",
    "      subtracting the mean and dividing by the standard deviation for each feature. This ensures that all features have a similar scale.\n",
    "     4.Compute the Covariance Matrix: Calculate the covariance matrix of the standardized data. This matrix contains information about\n",
    "       the relationships between different features.\n",
    "     5.Calculate Eigenvalues and Eigenvectors: Compute the eigenvalues and eigenvectors of the covariance matrix. These eigenvectors\n",
    "       represent the directions in the feature space along which the data varies the most (principal components), and the eigenvalues\n",
    "       indicate the amount of variance captured by each principal component.\n",
    "     6.Sort and Select Principal Components: Sort the eigenvectors in descending order based on their corresponding eigenvalues.\n",
    "       The principal components corresponding to the largest eigenvalues capture the most variance in the data. Decide on the number\n",
    "       of principal components you want to retain based on the level of dimensionality reduction you desire.\n",
    "     7.Create a Projection Matrix: Create a projection matrix using the selected principal components. This matrix represents the\n",
    "       transformation from the original feature space to the reduced-dimensional space.\n",
    "     8.Transform the Data: Multiply the standardized data by the projection matrix to obtain the reduced-dimensional representation\n",
    "       of the data.\n",
    "     9.Use Reduced-Dimensional Data for Modeling: The reduced-dimensional data obtained from PCA can now be used as input for your \n",
    "       stock price prediction model. This lower-dimensional representation simplifies the data, reduces noise, and can potentially \n",
    "       lead to better model performance and generalization.\n",
    "    10.Evaluate and Fine-Tune: Evaluate the performance of your stock price prediction model using the reduced-dimensional data and\n",
    "       fine-tune your model as needed. You can compare the performance with and without PCA to determine its effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbfdbcd-8285-4e11-9c61-3d3c81e8c944",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range \n",
    "    of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4724d330-cd7d-4bfe-8d43-40c6d3ffe230",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer : the Min-Max scaling formula to each value in the dataset to map it within the range of -1 to 1:\n",
    "         X_scaled = -1 + 2 * (X - X_min) / (X_max - X_min)\n",
    "        \n",
    "      For 1: X_scaled = -1 + 2 * (1 - 1) / (20 - 1) = -1\n",
    "      For 5: X_scaled = -1 + 2 * (5 - 1) / (20 - 1) = -0.6\n",
    "     For 10: X_scaled = -1 + 2 * (10 - 1) / (20 - 1) = -0.2\n",
    "     For 15: X_scaled = -1 + 2 * (15 - 1) / (20 - 1) = 0.2\n",
    "     For 20: X_scaled = -1 + 2 * (20 - 1) / (20 - 1) = 1\n",
    "    \n",
    "     Scaled value : [-1.0, -0.6, -0.2, 0.2, 1.0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472c0395-6ac5-4f5d-be4d-dbf3ba693f32",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction \n",
    "    using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3287b8-9112-4731-ab01-e5bafec0a39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer : The decision of how many principal components to retain in PCA depends on the goals of your analysis, the amount of \n",
    "         variance you want to preserve, and the trade-off between reducing dimensionality and retaining meaningful information. \n",
    "         To decide on the number of principal components to retain, you can consider the cumulative explained variance and potentially\n",
    "         use methods like the elbow method or scree plot.\n",
    "            \n",
    "             the goal is to retain a significant amount of variance while reducing the dimensionality. There's no one-size-fits-all \n",
    "        answer, so it's important to consider the specific characteristics of your data and the goals of your analysis when deciding on\n",
    "        the number of principal components to retain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
